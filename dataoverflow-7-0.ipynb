{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14913415,"datasetId":9542524,"databundleVersionId":15779325},{"sourceType":"datasetVersion","sourceId":14915146,"datasetId":9543525,"databundleVersionId":15781167}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nimport warnings\nimport random\nimport joblib\nwarnings.filterwarnings(\"ignore\")\n\n# ── Global seed — change this one value to reproduce any run ────────────────\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:44.427022Z","iopub.execute_input":"2026-02-22T03:26:44.427790Z","iopub.status.idle":"2026-02-22T03:26:44.433134Z","shell.execute_reply.started":"2026-02-22T03:26:44.427753Z","shell.execute_reply":"2026-02-22T03:26:44.432328Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 1. Load Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/datasets/yassinechelly4/dataoverflow2/train.csv\")\ndf_test  = pd.read_csv(\"/kaggle/input/datasets/yassinechelly4/dataoverflow2/test.csv\")\n\nTARGET   = \"Purchased_Coverage_Bundle\"\ntest_ids = df_test[\"User_ID\"].copy()\n\n# ── Columns to drop\n# Policy_Cancelled_Post_Purchase: kept — has small but real signal (corr=0.03)\n# Broker_ID: dropped (8331 nulls) BUT we extract a binary flag first\n# Employer_ID: 57396/60868 null — nearly all missing, drop\n# Policy_Start_Week / Day: dropped — cyclically encoded or low signal\nDROP_COLS = [\"User_ID\", \"Employer_ID\", \"Policy_Start_Week\", \"Policy_Start_Day\"]\n\ndf_train = df_train.drop(columns=DROP_COLS, errors=\"ignore\")\ndf_test  = df_test.drop(columns=DROP_COLS, errors=\"ignore\")\nprint(f\"Train shape: {df_train.shape} | Test shape: {df_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:44.434532Z","iopub.execute_input":"2026-02-22T03:26:44.434771Z","iopub.status.idle":"2026-02-22T03:26:44.746476Z","shell.execute_reply.started":"2026-02-22T03:26:44.434749Z","shell.execute_reply":"2026-02-22T03:26:44.745644Z"}},"outputs":[{"name":"stdout","text":"Train shape: (60868, 25) | Test shape: (15218, 24)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 2. Feature Engineering\n\nAll features engineered from domain knowledge + correlation analysis. Applied identically to train and test before any encoding.","metadata":{}},{"cell_type":"code","source":"def engineer_features(df, is_train=True):\n    df = df.copy()\n\n    # ── Binary flags (extracted BEFORE imputation so missingness is captured)\n    df[\"Has_Broker\"]   = df[\"Broker_ID\"].notna().astype(int)\n    df[\"Has_Employer\"] = (df.get(\"Employer_ID\", pd.Series(np.nan, index=df.index)).notna()).astype(int)\n\n    # ── Impute Broker_ID with -1 (unknown) then keep as numeric signal\n    df[\"Broker_ID\"] = df[\"Broker_ID\"].fillna(-1)\n\n    # ── Dependent aggregates\n    df[\"Child_Dependents\"]    = df[\"Child_Dependents\"].fillna(0)\n    df[\"Total_Dependents\"]    = df[\"Adult_Dependents\"] + df[\"Child_Dependents\"] + df[\"Infant_Dependents\"]\n    df[\"Minor_Dependents\"]    = df[\"Child_Dependents\"] + df[\"Infant_Dependents\"]\n\n    # ── Income features\n    df[\"Log_Income\"]          = np.log1p(df[\"Estimated_Annual_Income\"])\n    df[\"Income_Per_Dependent\"]= df[\"Estimated_Annual_Income\"] / (df[\"Total_Dependents\"] + 1)\n    df[\"Income_Bracket\"]      = pd.qcut(df[\"Estimated_Annual_Income\"], q=5, labels=False, duplicates=\"drop\").astype(float)\n\n    # ── Policy risk features\n    df[\"Policy_Complexity\"]   = df[\"Vehicles_on_Policy\"] + df[\"Custom_Riders_Requested\"] + df[\"Policy_Amendments_Count\"]\n    df[\"Grace_Ratio\"]         = df[\"Grace_Period_Extensions\"] / (df[\"Previous_Policy_Duration_Months\"] + 1)\n    df[\"Claims_Rate\"]         = df[\"Previous_Claims_Filed\"] / (df[\"Previous_Policy_Duration_Months\"] + 1)\n    df[\"Clean_Ratio\"]         = df[\"Years_Without_Claims\"] / (df[\"Previous_Policy_Duration_Months\"] + 1)\n\n    # ── Deductible: this IS an ordered variable (tier 1 > tier 4)\n    ded_map = {\"Tier_1_High_Ded\": 3, \"Tier_2_Mid_Ded\": 2, \"Tier_3_Low_Ded\": 1, \"Tier_4_Zero_Ded\": 0}\n    df[\"Deductible_Ord\"] = df[\"Deductible_Tier\"].map(ded_map).fillna(-1)\n    df = df.drop(columns=[\"Deductible_Tier\"])\n\n    # ── Cyclical month encoding (preserves Jan-Dec circularity)\n    month_map = {\"January\":1,\"February\":2,\"March\":3,\"April\":4,\"May\":5,\"June\":6,\n                 \"July\":7,\"August\":8,\"September\":9,\"October\":10,\"November\":11,\"December\":12}\n    df[\"Month_Num\"] = df[\"Policy_Start_Month\"].map(month_map).fillna(0)\n    df[\"Month_Sin\"] = np.sin(2 * np.pi * df[\"Month_Num\"] / 12)\n    df[\"Month_Cos\"] = np.cos(2 * np.pi * df[\"Month_Num\"] / 12)\n    df = df.drop(columns=[\"Policy_Start_Month\", \"Month_Num\"])\n\n    return df\n\ndf_train = engineer_features(df_train)\ndf_test  = engineer_features(df_test)\nprint(f\"Features after engineering: {df_train.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:44.747530Z","iopub.execute_input":"2026-02-22T03:26:44.747871Z","iopub.status.idle":"2026-02-22T03:26:44.847433Z","shell.execute_reply.started":"2026-02-22T03:26:44.747847Z","shell.execute_reply":"2026-02-22T03:26:44.846453Z"}},"outputs":[{"name":"stdout","text":"Features after engineering: 37\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 3. Imputation & Encoding","metadata":{}},{"cell_type":"code","source":"# ── Identify column types (after engineering)\nnum_cols = df_train.select_dtypes(include=[np.number]).columns.drop(TARGET, errors=\"ignore\").tolist()\n# Remaining categorical cols (Deductible_Tier already ordinal-encoded above)\ncat_cols = [c for c in df_train.select_dtypes(include=[\"object\"]).columns\n            if c not in [TARGET, \"Region_Code\"]]\n\nprint(\"Numeric cols :\", len(num_cols))\nprint(\"Nominal cats :\", cat_cols)\nprint(\"High-card cat: Region_Code (166 unique) → target encoding\")\n\n# ── Numeric imputation with TRAIN medians (applied to both)\nmedians = df_train[num_cols].median()\ndf_train[num_cols] = df_train[num_cols].fillna(medians)\ndf_test[num_cols]  = df_test[num_cols].fillna(medians)\n\n# ── Nominal categoricals: mode imputation then OHE\nfor col in cat_cols:\n    mode_val = df_train[col].mode()[0]\n    df_train[col] = df_train[col].fillna(mode_val)\n    df_test[col]  = df_test[col].fillna(mode_val)\n\n# NOTE: OrdinalEncoder assigns arbitrary integers to unordered categories.\n# For XGBoost, One-Hot Encoding is strictly better for nominal cats\n# because OE implies a false ordering (e.g. Urban_Boutique=0 < National_Corporate=1).\ndf_train = pd.get_dummies(df_train, columns=cat_cols, drop_first=False, dtype=int)\ndf_test  = pd.get_dummies(df_test,  columns=cat_cols, drop_first=False, dtype=int)\n\n# ── Align columns (test may be missing some OHE columns from rare train values)\ndf_test = df_test.reindex(columns=df_train.drop(columns=[TARGET]).columns, fill_value=0)\n\n# ── Target encoding for Region_Code (high-cardinality: 166 unique values)\n# OrdinalEncoder on 166 unordered countries is very noisy; target encoding\n# captures the actual mean outcome per region with Bayesian smoothing.\ndf_train[\"Region_Code\"] = df_train[\"Region_Code\"].fillna(\"Unknown\")\ndf_test[\"Region_Code\"]  = df_test[\"Region_Code\"].fillna(\"Unknown\")\n\nglobal_mean = df_train[TARGET].mean()\nsmoothing   = 20\nregion_stats = df_train.groupby(\"Region_Code\")[TARGET].agg([\"mean\",\"count\"])\nregion_stats[\"encoded\"] = (\n    (region_stats[\"mean\"] * region_stats[\"count\"] + global_mean * smoothing)\n    / (region_stats[\"count\"] + smoothing)\n)\nregion_map = region_stats[\"encoded\"].to_dict()\ndf_train[\"Region_Enc\"] = df_train[\"Region_Code\"].map(region_map).fillna(global_mean)\ndf_test[\"Region_Enc\"]  = df_test[\"Region_Code\"].map(region_map).fillna(global_mean)\ndf_train = df_train.drop(columns=[\"Region_Code\"])\ndf_test  = df_test.drop(columns=[\"Region_Code\"])\n\nprint(f\"Final feature count: {df_train.shape[1] - 1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:44.849236Z","iopub.execute_input":"2026-02-22T03:26:44.849561Z","iopub.status.idle":"2026-02-22T03:26:45.117839Z","shell.execute_reply.started":"2026-02-22T03:26:44.849536Z","shell.execute_reply":"2026-02-22T03:26:45.116965Z"}},"outputs":[{"name":"stdout","text":"Numeric cols : 31\nNominal cats : ['Broker_Agency_Type', 'Acquisition_Channel', 'Payment_Schedule', 'Employment_Status']\nHigh-card cat: Region_Code (166 unique) → target encoding\nFinal feature count: 46\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 4. Prepare X / y\n\n> **Note:** StandardScaler removed. XGBoost is tree-based — it uses threshold comparisons on raw feature values. Scaling has literally zero effect on any split decision and only adds preprocessing complexity and a potential train/test leakage vector.","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(columns=[TARGET])\ny = df_train[TARGET].values.astype(int)\nX_test_final = df_test.copy()\n\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\nNUM_CLASSES = len(le.classes_)\nprint(f\"Classes: {le.classes_}  ({NUM_CLASSES} total)\")\nprint(f\"\\nClass distribution:\\n{pd.Series(y_enc).value_counts().sort_index()}\")\nfeature_names = list(X.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:45.118665Z","iopub.execute_input":"2026-02-22T03:26:45.118902Z","iopub.status.idle":"2026-02-22T03:26:45.137019Z","shell.execute_reply.started":"2026-02-22T03:26:45.118880Z","shell.execute_reply":"2026-02-22T03:26:45.136220Z"}},"outputs":[{"name":"stdout","text":"Classes: [0 1 2 3 4 5 6 7 8 9]  (10 total)\n\nClass distribution:\n0      823\n1     1625\n2    36136\n3     4831\n4    13958\n5      479\n6      719\n7     2286\n8        6\n9        5\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 5. Train / Validation Split","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(\n    X.values, y_enc,\n    test_size=0.1, random_state=SEED, stratify=y_enc\n)\nprint(f\"Train: {X_train.shape} | Val: {X_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:45.138125Z","iopub.execute_input":"2026-02-22T03:26:45.138693Z","iopub.status.idle":"2026-02-22T03:26:45.208632Z","shell.execute_reply.started":"2026-02-22T03:26:45.138661Z","shell.execute_reply":"2026-02-22T03:26:45.207957Z"}},"outputs":[{"name":"stdout","text":"Train: (54781, 46) | Val: (6087, 46)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 6. Balanced Resampling — SMOTETomek (targeted)\n\n**Problem with the original approach:**\nThe original SMOTE resampled ALL classes up to the majority class size (32,522).\nClasses 8 and 9 had only 5–6 real samples — after resampling, **98% of their training data was purely synthetic**, completely hallucinated by KNN interpolation.\nThis causes the model to memorise synthetic artefacts, not real patterns.\n\n**Fix — two-part strategy:**\n1. **Moderate oversampling** of rare classes (capped at 200 samples, not 32,522)\n2. **Tomek Links cleaning** removes borderline majority samples near minority boundaries, giving cleaner decision boundaries without inflating data volume.\n3. **XGBoost ** handles the remaining imbalance at training time without synthetic data at all.","metadata":{}},{"cell_type":"code","source":"print(\"Before resampling:\", pd.Series(y_train).value_counts().sort_index().to_dict())\n\n# Count per class — only oversample classes that are severely under-represented\nclass_counts = pd.Series(y_train).value_counts()\ntarget_count  = 500   # oversample tiny classes to at most 500 (not 32522)\n\nsampling_strategy = {}\nfor cls, cnt in class_counts.items():\n    if cnt < target_count:\n        sampling_strategy[cls] = min(target_count, cnt * 50)  # cap at 50x original\n\nprint(f\"SMOTE sampling targets: {sampling_strategy}\")\n\nk_neighbors = max(1, min(5, class_counts.min() - 1))\nsmote = SMOTE(random_state=SEED, k_neighbors=k_neighbors, sampling_strategy=sampling_strategy)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\nprint(\"After resampling:\", pd.Series(y_train_res).value_counts().sort_index().to_dict())\nprint(f\"Train size: {len(X_train_res):,}  (was {len(X_train):,})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:45.209986Z","iopub.execute_input":"2026-02-22T03:26:45.210280Z","iopub.status.idle":"2026-02-22T03:26:45.260117Z","shell.execute_reply.started":"2026-02-22T03:26:45.210255Z","shell.execute_reply":"2026-02-22T03:26:45.259379Z"}},"outputs":[{"name":"stdout","text":"Before resampling: {0: 741, 1: 1463, 2: 32522, 3: 4348, 4: 12562, 5: 431, 6: 647, 7: 2057, 8: 5, 9: 5}\nSMOTE sampling targets: {5: 500, 8: 250, 9: 250}\nAfter resampling: {0: 741, 1: 1463, 2: 32522, 3: 4348, 4: 12562, 5: 500, 6: 647, 7: 2057, 8: 250, 9: 250}\nTrain size: 55,340  (was 54,781)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 7. Class Weights for XGBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_sample_weight\n\n# Compute per-sample weights from resampled labels\n# This handles remaining imbalance at the loss level — no fake data needed\nsample_weights = compute_sample_weight(class_weight=\"balanced\", y=y_train_res)\nprint(f\"Sample weight range: {sample_weights.min():.4f} – {sample_weights.max():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:26:45.261299Z","iopub.execute_input":"2026-02-22T03:26:45.261635Z","iopub.status.idle":"2026-02-22T03:26:45.276262Z","shell.execute_reply.started":"2026-02-22T03:26:45.261602Z","shell.execute_reply":"2026-02-22T03:26:45.275255Z"}},"outputs":[{"name":"stdout","text":"Sample weight range: 0.1702 – 22.1360\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 8. Train XGBoost\n\n**Key improvements:**\n- : stops training when val loss stops improving (the original had NO early stopping — val loss was still decreasing at tree 299, meaning it was undertrained)\n-  passed to fit instead of pure SMOTE inflation\n- Removed redundant  (no effect on tree models)\n- Added  to track both loss and error","metadata":{}},{"cell_type":"code","source":"xgb_model = xgb.XGBClassifier(\n    objective         = \"multi:softprob\",\n    num_class         = NUM_CLASSES,\n    eval_metric       = [\"mlogloss\", \"merror\"],\n    n_estimators      = 1000,\n    max_depth         = 5,\n    learning_rate     = 0.1,\n    subsample         = 0.8,\n    colsample_bytree  = 0.8,\n    random_state      = SEED,\n    n_jobs            = -1,\n    verbosity         = 0,\n    early_stopping_rounds = 50,\n)\nxgb_model.fit(\n    X_train_res, y_train_res,\n    sample_weight     = sample_weights,\n    eval_set          = [(X_val, y_val)],\n    verbose           = 50,\n)\nprint(f\"\\nBest iteration: {xgb_model.best_iteration}\")\nprint(f\"Best val mlogloss: {xgb_model.best_score:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:27:34.394234Z","iopub.execute_input":"2026-02-22T03:27:34.394576Z","iopub.status.idle":"2026-02-22T03:28:13.438130Z","shell.execute_reply.started":"2026-02-22T03:27:34.394548Z","shell.execute_reply":"2026-02-22T03:28:13.437504Z"}},"outputs":[{"name":"stdout","text":"[0]\tvalidation_0-mlogloss:2.16036\tvalidation_0-merror:0.55890\n[50]\tvalidation_0-mlogloss:1.11456\tvalidation_0-merror:0.44932\n[100]\tvalidation_0-mlogloss:0.99891\tvalidation_0-merror:0.42057\n[150]\tvalidation_0-mlogloss:0.93635\tvalidation_0-merror:0.39494\n[200]\tvalidation_0-mlogloss:0.89421\tvalidation_0-merror:0.37539\n[250]\tvalidation_0-mlogloss:0.86174\tvalidation_0-merror:0.36028\n[300]\tvalidation_0-mlogloss:0.83748\tvalidation_0-merror:0.35058\n[350]\tvalidation_0-mlogloss:0.81575\tvalidation_0-merror:0.33843\n[400]\tvalidation_0-mlogloss:0.80022\tvalidation_0-merror:0.33087\n[450]\tvalidation_0-mlogloss:0.78521\tvalidation_0-merror:0.32282\n[500]\tvalidation_0-mlogloss:0.77322\tvalidation_0-merror:0.31493\n[550]\tvalidation_0-mlogloss:0.76255\tvalidation_0-merror:0.31017\n[600]\tvalidation_0-mlogloss:0.75444\tvalidation_0-merror:0.30754\n[650]\tvalidation_0-mlogloss:0.74655\tvalidation_0-merror:0.30409\n[700]\tvalidation_0-mlogloss:0.73851\tvalidation_0-merror:0.29883\n[750]\tvalidation_0-mlogloss:0.73319\tvalidation_0-merror:0.29752\n[781]\tvalidation_0-mlogloss:0.73013\tvalidation_0-merror:0.29752\n\nBest iteration: 731\nBest val mlogloss: 0.29621\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## 9. Evaluate","metadata":{}},{"cell_type":"code","source":"y_pred = xgb_model.predict(X_val)\n\nprint(\"Accuracy :\", round(accuracy_score(y_val, y_pred), 4))\nprint(\"Macro F1  :\", round(f1_score(y_val, y_pred, average=\"macro\",    zero_division=0), 4))\nprint(\"Weighted F1:\",round(f1_score(y_val, y_pred, average=\"weighted\", zero_division=0), 4))\nprint()\nprint(classification_report(y_val, y_pred, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:28:34.263576Z","iopub.execute_input":"2026-02-22T03:28:34.263894Z","iopub.status.idle":"2026-02-22T03:28:34.730076Z","shell.execute_reply.started":"2026-02-22T03:28:34.263862Z","shell.execute_reply":"2026-02-22T03:28:34.729274Z"}},"outputs":[{"name":"stdout","text":"Accuracy : 0.7038\nMacro F1  : 0.5508\nWeighted F1: 0.7204\n\n              precision    recall  f1-score   support\n\n           0       0.42      0.76      0.54        82\n           1       0.61      0.69      0.65       162\n           2       0.91      0.74      0.82      3614\n           3       0.36      0.66      0.46       483\n           4       0.59      0.62      0.61      1396\n           5       0.68      0.71      0.69        48\n           6       0.54      0.61      0.58        72\n           7       0.53      0.72      0.61       229\n           8       0.00      0.00      0.00         1\n\n    accuracy                           0.70      6087\n   macro avg       0.52      0.61      0.55      6087\nweighted avg       0.76      0.70      0.72      6087\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 10. Feature Importance (Top 20)","metadata":{}},{"cell_type":"code","source":"import matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nimportance = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\nprint(\"Top 20 features:\")\nprint(importance.head(20).to_string())\n\nfig, ax = plt.subplots(figsize=(10, 7))\nimportance.head(20).sort_values().plot.barh(ax=ax)\nax.set_title(\"XGBoost Feature Importance — Top 20\")\nax.set_xlabel(\"Importance Score\")\nplt.tight_layout()\nplt.savefig(\"feature_importance.png\", dpi=120)\nplt.close()\nprint(\"Saved feature_importance.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:28:41.684063Z","iopub.execute_input":"2026-02-22T03:28:41.684406Z","iopub.status.idle":"2026-02-22T03:28:42.170662Z","shell.execute_reply.started":"2026-02-22T03:28:41.684379Z","shell.execute_reply":"2026-02-22T03:28:42.169905Z"}},"outputs":[{"name":"stdout","text":"Top 20 features:\nTotal_Dependents                         0.124701\nBroker_Agency_Type_Urban_Boutique        0.119457\nBroker_Agency_Type_National_Corporate    0.101011\nMinor_Dependents                         0.068286\nIncome_Bracket                           0.062473\nHas_Broker                               0.050862\nAdult_Dependents                         0.044284\nPolicy_Start_Year                        0.038655\nChild_Dependents                         0.037519\nAcquisition_Channel_Direct_Website       0.034685\nBroker_ID                                0.027283\nAcquisition_Channel_Affiliate_Group      0.015240\nAcquisition_Channel_Corporate_Partner    0.015079\nDeductible_Ord                           0.015042\nMonth_Sin                                0.014764\nEmployment_Status_Self_Employed          0.014628\nAcquisition_Channel_Aggregator_Site      0.014125\nRegion_Enc                               0.012329\nEmployment_Status_Employed_FullTime      0.011942\nLog_Income                               0.011660\nSaved feature_importance.png\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 11. Generate Submission","metadata":{}},{"cell_type":"code","source":"y_test_pred   = xgb_model.predict(X_test_final.values)\ny_test_labels = le.inverse_transform(y_test_pred)\n\nsubmission = pd.DataFrame({\n    \"User_ID\": test_ids,\n    TARGET:    y_test_labels\n})\nsubmission.to_csv(\"xgb_submission.csv\", index=False)\nprint(f\"Submission saved — {len(submission):,} rows\")\nprint(submission.head(10).to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:28:46.345044Z","iopub.execute_input":"2026-02-22T03:28:46.345420Z","iopub.status.idle":"2026-02-22T03:28:47.460546Z","shell.execute_reply.started":"2026-02-22T03:28:46.345390Z","shell.execute_reply":"2026-02-22T03:28:47.459764Z"}},"outputs":[{"name":"stdout","text":"Submission saved — 15,218 rows\n   User_ID  Purchased_Coverage_Bundle\nUSR_060868                          0\nUSR_060869                          2\nUSR_060870                          2\nUSR_060871                          4\nUSR_060872                          2\nUSR_060873                          4\nUSR_060874                          4\nUSR_060875                          6\nUSR_060876                          3\nUSR_060877                          2\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## 12. Save Full Artifact","metadata":{}},{"cell_type":"code","source":"artifact = {\n    \"model\":           xgb_model,\n    \"feature_names\":   feature_names,\n    \"label_encoder\":   le,\n    \"region_map\":      region_map,\n    \"global_mean\":     global_mean,\n    \"medians\":         medians.to_dict(),\n    \"cat_cols\":        cat_cols,\n    \"num_classes\":     NUM_CLASSES,\n}\njoblib.dump(artifact, \"model.joblib\")\nprint(\"Artifact saved → model.joblib\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T03:28:57.249460Z","iopub.execute_input":"2026-02-22T03:28:57.250114Z","iopub.status.idle":"2026-02-22T03:28:57.379449Z","shell.execute_reply.started":"2026-02-22T03:28:57.250078Z","shell.execute_reply":"2026-02-22T03:28:57.378742Z"}},"outputs":[{"name":"stdout","text":"Artifact saved → model.joblib\n","output_type":"stream"}],"execution_count":24}]}