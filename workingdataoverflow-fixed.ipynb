{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:21.933565Z",
     "iopub.status.busy": "2026-02-22T00:29:21.933186Z",
     "iopub.status.idle": "2026-02-22T00:29:21.937925Z",
     "shell.execute_reply": "2026-02-22T00:29:21.937107Z",
     "shell.execute_reply.started": "2026-02-22T00:29:21.933535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install exact versions (force reinstall to avoid conflicts)\n",
    "\n",
    "#!pip install --no-cache-dir --force-reinstall \\\n",
    "#numpy==1.26.4 \\\n",
    "#scipy==1.11.4 \\\n",
    "#pandas==2.1.4 \\\n",
    "#scikit-learn==1.3.2 \\\n",
    "#xgboost==2.0.3 \\\n",
    "#lightgbm==4.6.0 \\\n",
    "#catboost==1.2.3 \\\n",
    "#torch==2.6.0 \\\n",
    "#torchvision==0.21.0 \\\n",
    "#torchaudio==2.6.0 \\\n",
    "#joblib==1.3.2 \\\n",
    "#tqdm==4.66.1 \\\n",
    "#pyyaml==6.0.1 \\\n",
    "#cloudpickle==3.0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:21.940219Z",
     "iopub.status.busy": "2026-02-22T00:29:21.939839Z",
     "iopub.status.idle": "2026-02-22T00:29:21.955590Z",
     "shell.execute_reply": "2026-02-22T00:29:21.954504Z",
     "shell.execute_reply.started": "2026-02-22T00:29:21.940190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:21.957147Z",
     "iopub.status.busy": "2026-02-22T00:29:21.956796Z",
     "iopub.status.idle": "2026-02-22T00:29:21.973848Z",
     "shell.execute_reply": "2026-02-22T00:29:21.972710Z",
     "shell.execute_reply.started": "2026-02-22T00:29:21.957113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"/kaggle/input/datasets/yassinechelly4/dataoverflow/train.csv\"\n",
    "TEST_PATH  = \"/kaggle/input/datasets/yassinechelly4/dataoverflow/test.csv\"\n",
    "MODEL_PATH = \"catboost_model.joblib\"\n",
    "SEED = 42\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:21.975711Z",
     "iopub.status.busy": "2026-02-22T00:29:21.975297Z",
     "iopub.status.idle": "2026-02-22T00:29:21.993828Z",
     "shell.execute_reply": "2026-02-22T00:29:21.992651Z",
     "shell.execute_reply.started": "2026-02-22T00:29:21.975682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Implement any preprocessing steps required for your model here.\n",
    "    # Return a Pandas DataFrame of the data\n",
    "    #\n",
    "    # Note: Don't drop the 'User_ID' column here.\n",
    "    # It will be used in the predict function to return the final predictions.\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # \u2500\u2500 1. DROP LOW-SIGNAL COLUMNS (keep User_ID) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    DROP_COLS = [\n",
    "        \"Employer_ID\",\n",
    "        \"Previous_Claims_Filed\",\n",
    "        \"Existing_Policyholder\",\n",
    "        \"Underwriting_Processing_Days\",\n",
    "        \"Infant_Dependents\",\n",
    "        \"Policy_Start_Day\",\n",
    "    ]\n",
    "    df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True)\n",
    "\n",
    "    # \u2500\u2500 2. MISSING VALUE HANDLING \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    df[\"Child_Dependents\"]  = df[\"Child_Dependents\"].fillna(0)\n",
    "    df[\"Has_Broker\"]        = df[\"Broker_ID\"].notna().astype(int)\n",
    "    df[\"Broker_ID\"]         = df[\"Broker_ID\"].fillna(-1)\n",
    "    df[\"Region_Code\"]       = df[\"Region_Code\"].fillna(\"Unknown\")\n",
    "    df[\"Deductible_Tier\"]   = df[\"Deductible_Tier\"].fillna(\"Unknown\")\n",
    "    df[\"Acquisition_Channel\"] = df[\"Acquisition_Channel\"].fillna(\"Unknown\")\n",
    "\n",
    "    # \u2500\u2500 3. FEATURE ENGINEERING \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    df[\"Total_Dependents\"]      = df[\"Adult_Dependents\"] + df[\"Child_Dependents\"]\n",
    "    df[\"Income_Per_Dependent\"]  = df[\"Estimated_Annual_Income\"] / (df[\"Total_Dependents\"] + 1)\n",
    "    df[\"Grace_To_Duration_Ratio\"] = df[\"Grace_Period_Extensions\"] / (df[\"Previous_Policy_Duration_Months\"] + 1)\n",
    "    df[\"Log_Income\"]            = np.log1p(df[\"Estimated_Annual_Income\"])\n",
    "    df[\"Log_Days_Since_Quote\"]  = np.log1p(df[\"Days_Since_Quote\"])\n",
    "\n",
    "    # Cyclical month encoding\n",
    "    month_order = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "                   \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "    df[\"Month_Num\"] = pd.Categorical(df[\"Policy_Start_Month\"], categories=month_order, ordered=True).codes + 1\n",
    "    df[\"Month_Sin\"] = np.sin(2 * np.pi * df[\"Month_Num\"] / 12)\n",
    "    df[\"Month_Cos\"] = np.cos(2 * np.pi * df[\"Month_Num\"] / 12)\n",
    "    df.drop(columns=[\"Policy_Start_Month\", \"Month_Num\"], inplace=True)\n",
    "\n",
    "    # \u2500\u2500 4. ENCODING \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # Ordinal: Deductible_Tier\n",
    "    deductible_map = {\n",
    "        \"Tier_1_High_Ded\": 3,\n",
    "        \"Tier_2_Mid_Ded\":  2,\n",
    "        \"Tier_3_Low_Ded\":  1,\n",
    "        \"Tier_4_Zero_Ded\": 0,\n",
    "        \"Unknown\":        -1,\n",
    "    }\n",
    "    df[\"Deductible_Tier\"] = df[\"Deductible_Tier\"].map(deductible_map)\n",
    "\n",
    "    # Target encoding: Region_Code (high cardinality \u2014 166 unique values)\n",
    "    if \"Purchased_Coverage_Bundle\" in df.columns:\n",
    "        global_mean = df[\"Purchased_Coverage_Bundle\"].mean()\n",
    "        smoothing   = 10\n",
    "        stats = df.groupby(\"Region_Code\")[\"Purchased_Coverage_Bundle\"].agg([\"mean\", \"count\"])\n",
    "        stats[\"encoded\"] = (\n",
    "            (stats[\"mean\"] * stats[\"count\"] + global_mean * smoothing) /\n",
    "            (stats[\"count\"] + smoothing)\n",
    "        )\n",
    "        df[\"Region_Code_Encoded\"] = df[\"Region_Code\"].map(stats[\"encoded\"]).fillna(global_mean)\n",
    "    else:\n",
    "        # Test set: fall back to global mean (0.0 placeholder \u2014 replace with train map in predict)\n",
    "        df[\"Region_Code_Encoded\"] = 0.0\n",
    "    df.drop(columns=[\"Region_Code\"], inplace=True)\n",
    "\n",
    "    # One-hot encoding: low-cardinality categoricals\n",
    "    OHE_COLS = [\"Broker_Agency_Type\", \"Acquisition_Channel\", \"Payment_Schedule\", \"Employment_Status\"]\n",
    "    df = pd.get_dummies(df, columns=OHE_COLS, drop_first=False, dtype=int)\n",
    "\n",
    "    # \u2500\u2500 5. STANDARDISATION (StandardScaler \u2014 continuous/skewed features) \u2500\u2500\u2500\u2500\n",
    "    STD_COLS = [c for c in [\n",
    "        \"Estimated_Annual_Income\", \"Log_Income\", \"Log_Days_Since_Quote\",\n",
    "        \"Income_Per_Dependent\", \"Grace_To_Duration_Ratio\", \"Days_Since_Quote\",\n",
    "        \"Previous_Policy_Duration_Months\", \"Policy_Start_Year\",\n",
    "        \"Policy_Start_Week\", \"Broker_ID\", \"Region_Code_Encoded\",\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    df[STD_COLS] = std_scaler.fit_transform(df[STD_COLS])\n",
    "\n",
    "    # \u2500\u2500 6. NORMALISATION (MinMaxScaler \u2014 counts/bounded features) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    MM_COLS = [c for c in [\n",
    "        \"Adult_Dependents\", \"Child_Dependents\", \"Total_Dependents\",\n",
    "        \"Grace_Period_Extensions\", \"Years_Without_Claims\",\n",
    "        \"Policy_Amendments_Count\", \"Vehicles_on_Policy\",\n",
    "        \"Custom_Riders_Requested\", \"Deductible_Tier\",\n",
    "        \"Month_Sin\", \"Month_Cos\",\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    df[MM_COLS] = mm_scaler.fit_transform(df[MM_COLS])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:21.996428Z",
     "iopub.status.busy": "2026-02-22T00:29:21.995996Z",
     "iopub.status.idle": "2026-02-22T00:29:22.014986Z",
     "shell.execute_reply": "2026-02-22T00:29:22.014008Z",
     "shell.execute_reply.started": "2026-02-22T00:29:21.996399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = None\n",
    "    # ------------------ MODEL LOADING LOGIC ------------------\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    # ------------------ END MODEL LOADING LOGIC ------------------\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model predection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:22.016527Z",
     "iopub.status.busy": "2026-02-22T00:29:22.016155Z",
     "iopub.status.idle": "2026-02-22T00:29:22.031853Z",
     "shell.execute_reply": "2026-02-22T00:29:22.030603Z",
     "shell.execute_reply.started": "2026-02-22T00:29:22.016493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(df, model):\n",
    "    predictions = None\n",
    "    # ------------------ PREDICTION LOGIC ------------------\n",
    "    # df has already been passed through preprocess().\n",
    "    # User_ID is retained in df for the final output.\n",
    "\n",
    "    user_ids = df[\"User_ID\"]\n",
    "    X = df.drop(columns=[\"User_ID\"], errors=\"ignore\")\n",
    "\n",
    "    # Drop target column if accidentally present (e.g. during local testing)\n",
    "    X = X.drop(columns=[\"Purchased_Coverage_Bundle\"], errors=\"ignore\")\n",
    "\n",
    "    # Align test columns to exactly match training feature set\n",
    "    # (adds any missing OHE columns as 0, drops any extras)\n",
    "    train_features = model.feature_names_\n",
    "    X = X.reindex(columns=train_features, fill_value=0)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    preds = preds.flatten()\n",
    "\n",
    "    predictions = pd.DataFrame({\n",
    "        \"User_ID\":                   user_ids.values,\n",
    "        \"Purchased_Coverage_Bundle\": preds,\n",
    "    })\n",
    "    # ------------------ END PREDICTION LOGIC ------------------\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:22.039608Z",
     "iopub.status.busy": "2026-02-22T00:29:22.038758Z",
     "iopub.status.idle": "2026-02-22T00:29:22.048359Z",
     "shell.execute_reply": "2026-02-22T00:29:22.047442Z",
     "shell.execute_reply.started": "2026-02-22T00:29:22.039576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "def run(df) -> tuple[float, float, float]:\n",
    "    # Load the processed data:\n",
    "    df_processed = preprocess(df)\n",
    "\n",
    "    # Extract true labels before they get dropped in predict()\n",
    "    true_labels = None\n",
    "    if \"Purchased_Coverage_Bundle\" in df.columns:\n",
    "        true_labels = df[\"Purchased_Coverage_Bundle\"].values\n",
    "\n",
    "    # Load the model:\n",
    "    model = load_model()\n",
    "    size = get_model_size(model)\n",
    "\n",
    "    # Get the predictions and time taken:\n",
    "    start = time.perf_counter()\n",
    "    predictions = predict(\n",
    "        df_processed, model\n",
    "    )  # NOTE: Don't call the `preprocess` function here.\n",
    "\n",
    "    duration = time.perf_counter() - start\n",
    "    macro_f1 = get_model_accuracy(predictions, true_labels)\n",
    "\n",
    "    return size, macro_f1, duration\n",
    "\n",
    "\n",
    "def get_model_size(model) -> float:\n",
    "    \"\"\"Return the serialised size of the model in megabytes.\"\"\"\n",
    "    return len(pickle.dumps(model)) / (1024 * 1024)\n",
    "\n",
    "\n",
    "def get_model_accuracy(predictions, true_labels=None) -> float:\n",
    "    \"\"\"Return Macro F1-Score if true labels are available, else 0.0.\"\"\"\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    if true_labels is None:\n",
    "        return 0.0\n",
    "\n",
    "    pred_labels = predictions[\"Purchased_Coverage_Bundle\"].values\n",
    "    return f1_score(true_labels, pred_labels, average=\"macro\")\n",
    "\n",
    "\n",
    "def compute_final_score(size_mb: float, macro_f1: float, latency_s: float) -> float:\n",
    "    \"\"\"\n",
    "    Replicates the hackathon scoring formula:\n",
    "        final_score = Macro F1\n",
    "                      \u00d7 max(0.5, 1 - size_mb / 200)   # Size Penalty\n",
    "                      \u00d7 max(0.5, 1 - latency_s / 10)   # Latency Penalty\n",
    "    \"\"\"\n",
    "    size_penalty    = max(0.5, 1 - size_mb   / 200)\n",
    "    latency_penalty = max(0.5, 1 - latency_s / 10)\n",
    "    return macro_f1 * size_penalty * latency_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:29:22.050373Z",
     "iopub.status.busy": "2026-02-22T00:29:22.049989Z",
     "iopub.status.idle": "2026-02-22T00:30:02.936616Z",
     "shell.execute_reply": "2026-02-22T00:30:02.935820Z",
     "shell.execute_reply.started": "2026-02-22T00:29:22.050346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data ...\n",
      "Preprocessing ...\n",
      "Training Logistic Regression ...\n",
      "\n",
      "Validation accuracy          : 0.5002\n",
      "Validation balanced accuracy : 0.5318\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.77      0.14       123\n",
      "           1       0.31      0.29      0.30       244\n",
      "           2       0.87      0.51      0.65      5421\n",
      "           3       0.23      0.51      0.32       724\n",
      "           4       0.52      0.46      0.49      2094\n",
      "           5       0.21      0.76      0.33        72\n",
      "           6       0.16      0.54      0.24       108\n",
      "           7       0.41      0.47      0.44       343\n",
      "           8       0.06      1.00      0.12         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50      9131\n",
      "   macro avg       0.29      0.53      0.30      9131\n",
      "weighted avg       0.68      0.50      0.55      9131\n",
      "\n",
      "\n",
      "Model saved \u2192 logistic_model.pkl\n"
     ]
    }
   ],
   "source": [
    "## \u2500\u2500 TRAINING HELPER \u2014 CatBoost with GridSearchCV & Progress Tracking \u2500\u2500\n",
    "\n",
    "def train_and_save():\n",
    "    from sklearn.metrics import balanced_accuracy_score, classification_report, accuracy_score\n",
    "    print('Loading data...')\n",
    "    train_raw = pd.read_csv(TRAIN_PATH)\n",
    "    train_processed = preprocess(train_raw)\n",
    "    TARGET = 'Purchased_Coverage_Bundle'\n",
    "    X = train_processed.drop(columns=['User_ID', TARGET], errors='ignore')\n",
    "    y = train_processed[TARGET]\n",
    "    \n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.15, random_state=SEED, stratify=y)\n",
    "    \n",
    "    print('Tuning CatBoost with GridSearchCV...')\n",
    "    param_grid = {\n",
    "        'depth': [4, 6],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'iterations': [100]\n",
    "    }\n",
    "    \n",
    "    # Using verbose=100 in CatBoost allows tqdm-like progress logs every 100 iterations\n",
    "    base_model = CatBoostClassifier(\n",
    "        loss_function='MultiClass',\n",
    "        random_seed=SEED,\n",
    "        task_type='GPU',\n",
    "        verbose=100, \n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    \n",
    "    # verbose=3 in GridSearchCV provides detailed progress tracking for each fold/parameter combo\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model, \n",
    "        param_grid, \n",
    "        cv=3, \n",
    "        scoring='f1_macro', \n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "    \n",
    "    # We wrap the fit call in a simple print statement as GridSearchCV handles the rest internally\n",
    "    with tqdm(total=1, desc=\"Overall Grid Search\") as pbar:\n",
    "        grid_search.fit(X_tr, y_tr)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f'Best params: {grid_search.best_params_}')\n",
    "    model = grid_search.best_estimator_\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    print(f'Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}')\n",
    "    print(classification_report(y_val, y_pred, zero_division=0))\n",
    "    \n",
    "    joblib.dump(model, MODEL_PATH)\n",
    "    print(f'Model saved to {MODEL_PATH}')\n",
    "    return model\n",
    "\n",
    "trained_model = train_and_save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2500\u2500 EVALUATE + SUBMIT with Progress Tracking \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# Step 1: Evaluate run() on training data\n",
    "print(\"=\" * 50)\n",
    "print(\"  Running full pipeline evaluation on train set\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Reading training data...\")\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "with tqdm(total=3, desc=\"Evaluation Progress\") as pbar:\n",
    "    # Using a progress bar for the three main steps in evaluation\n",
    "    pbar.set_description(\"Preprocessing data\")\n",
    "    # We'll call the components of run() separately to show progress\n",
    "    df_processed = preprocess(train_raw)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    pbar.set_description(\"Loading model\")\n",
    "    model = load_model()\n",
    "    pbar.update(1)\n",
    "    \n",
    "    pbar.set_description(\"Generating predictions\")\n",
    "    size = get_model_size(model)\n",
    "    start = time.perf_counter()\n",
    "    predictions = predict(df_processed, model)\n",
    "    duration = time.perf_counter() - start\n",
    "    \n",
    "    true_labels = train_raw[\"Purchased_Coverage_Bundle\"].values\n",
    "    accuracy = get_model_accuracy(predictions, true_labels)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f\"\\n  Model size  : {size:.4f} MB\")\n",
    "print(f\"  Accuracy    : {accuracy:.4f}  (balanced)\")\n",
    "print(f\"  Pred time   : {duration:.4f} seconds\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 2: Generate predictions on test set & save submission\n",
    "print(\"\\nGenerating test predictions ...\")\n",
    "with tqdm(total=3, desc=\"Submission Generation\") as pbar:\n",
    "    pbar.set_description(\"Reading test data\")\n",
    "    test_raw = pd.read_csv(TEST_PATH)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    pbar.set_description(\"Preprocessing test data\")\n",
    "    test_processed = preprocess(test_raw)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    pbar.set_description(\"Predicting & Saving\")\n",
    "    model = load_model()\n",
    "    submission = predict(test_processed, model)\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f\"Submission saved \u2192 submission.csv  ({len(submission):,} rows)\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:30:02.939717Z",
     "iopub.status.busy": "2026-02-22T00:30:02.937648Z",
     "iopub.status.idle": "2026-02-22T00:30:03.586363Z",
     "shell.execute_reply": "2026-02-22T00:30:03.584903Z",
     "shell.execute_reply.started": "2026-02-22T00:30:02.939680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "  Running full pipeline evaluation on train set\n",
      "==================================================\n",
      "\n",
      "  Model size  : 0.0049 MB\n",
      "  Accuracy    : 0.3614  (balanced)\n",
      "  Pred time   : 0.0538 seconds\n",
      "==================================================\n",
      "\n",
      "Generating test predictions ...\n",
      "Submission saved \u2192 submission.csv  (15,218 rows)\n",
      "\n",
      "Sample predictions:\n",
      "   User_ID  Purchased_Coverage_Bundle\n",
      "USR_060868                          2\n",
      "USR_060869                          2\n",
      "USR_060870                          2\n",
      "USR_060871                          2\n",
      "USR_060872                          2\n",
      "USR_060873                          4\n",
      "USR_060874                          3\n",
      "USR_060875                          6\n",
      "USR_060876                          3\n",
      "USR_060877                          2\n"
     ]
    }
   ],
   "source": [
    "## \u2500\u2500 EVALUATE + SUBMIT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# Step 1: Evaluate run() on training data\n",
    "print(\"=\" * 50)\n",
    "print(\"  Running full pipeline evaluation on train set\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_raw        = pd.read_csv(TRAIN_PATH)\n",
    "size, accuracy, duration = run(train_raw)\n",
    "\n",
    "print(f\"\\n  Model size  : {size:.4f} MB\")\n",
    "print(f\"  Accuracy    : {accuracy:.4f}  (balanced)\")\n",
    "print(f\"  Pred time   : {duration:.4f} seconds\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 2: Generate predictions on test set & save submission\n",
    "print(\"\\nGenerating test predictions ...\")\n",
    "test_raw       = pd.read_csv(TEST_PATH)\n",
    "test_processed = preprocess(test_raw)\n",
    "model          = load_model()\n",
    "submission     = predict(test_processed, model)\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(f\"Submission saved \u2192 submission.csv  ({len(submission):,} rows)\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission.head(10).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15779385,
     "datasetId": 9542572,
     "sourceId": 14913472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31286,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}